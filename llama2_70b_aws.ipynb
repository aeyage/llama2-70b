{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sagemaker boto3 --quiet\n",
    "%pip install transformers datasets[s3] --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e9ba069ec4412fa87bad6021caefe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aiman Hakimi\\.cache\\huggingface\\hub\\models--meta-llama--Llama-2-70b-hf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5aed8708184b06b19a5a2b1b32f74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1110596e55c4462ea64f7ec8f76cf82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcd59467442464187cfbaa84351fefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub.hf_api import HfFolder\n",
    "\n",
    "# update the access token to download the tokenizer\n",
    "access_token = \"hf_insert-your-key-here\"\n",
    "HfFolder.save_token(access_token)\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_name = \"meta-llama/Llama-2-70b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "block_size = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/neuronx_distributed/data'\n",
    "print(f\"uploading training dataset to: {training_input_path}\")\n",
    "train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"uploaded data to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no. of processes per node\n",
    "PROCESSES_PER_NODE = 32\n",
    "# no. of instances within the cluster, change this if you want to tweak the instance_count parameter\n",
    "WORLD_SIZE = 32\n",
    "# global batch size\n",
    "GBS = 512\n",
    "# input sequence length\n",
    "SEQ_LEN = 4096\n",
    "# pipeline parallel degree\n",
    "PP_DEGREE = 8 # tensor parallel degree\n",
    "TP_DEGREE = 8\n",
    "# data paralell size\n",
    "DP = ((PROCESSES_PER_NODE * WORLD_SIZE / TP_DEGREE / PP_DEGREE))\n",
    "# batch size per model replica\n",
    "BS = ((GBS / DP))\n",
    "# number microbatches for pipeline execution. setting same as BS so each microbatch contains a single data sample\n",
    "NUM_MICROBATCHES = BS\n",
    "# no. of total steps for which to train model and adjusted to the step number when the loss function is approaching convergence\n",
    "MAX_STEPS = 1500\n",
    "# timeout in seconds for training. after this amount of time, sagemaker terminates the job regardless of its current status\n",
    "MAX_RUN = 2 * (24 * 60 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "hyperparameters[\"train_batch_size\"] = int(BS)\n",
    "hyperparameters[\"use_meta_device_init\"] = 1\n",
    "# path where sagemaker uploads the training data\n",
    "hyperparameters[\"training_dir\"] = \"/opt/ml/input/data/train\" \n",
    "# config file containing llama 70b configuration , change this for tweaking the number of parameters\n",
    "hyperparameters[\"training_config\"] = \"config.json\" \n",
    "\n",
    "hyperparameters[\"max_steps\"] = MAX_STEPS\n",
    "hyperparameters[\"seq_len\"] = SEQ_LEN\n",
    "hyperparameters[\"pipeline_parallel_size\"] = PP_DEGREE\n",
    "hyperparameters[\"tensor_parallel_size\"] = TP_DEGREE\n",
    "hyperparameters[\"num_microbatches\"] = int(NUM_MICROBATCHES)\n",
    "hyperparameters[\"lr\"] = 0.00015\n",
    "hyperparameters[\"min_lr\"] = 1e-05\n",
    "hyperparameters[\"beta1\"] = 0.9\n",
    "hyperparameters[\"beta2\"] = 0.95\n",
    "hyperparameters[\"weight_decay\"] = 0.1\n",
    "hyperparameters[\"warmup_steps\"] = 2000\n",
    "hyperparameters[\"constant_steps\"] = 0\n",
    "hyperparameters[\"use_zero1_optimizer\"] = 1\n",
    "# the tensorboard logs will be stored here and eventually pushed to S3.\n",
    "hyperparameters[\"tb_dir\"] = \"/opt/ml/checkpoints/tensorboard\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_image = f\"763104351884.dkr.ecr.{region_name}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.18.0-ubuntu20.04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# training job name\n",
    "job_name = f'llama-neuron-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "# checkpoint directory that contains the weights and other relevant data for the trained model\n",
    "checkpoint_s3_uri = \"s3://\" + sagemaker_session_bucket + \"/neuron_llama_experiment\"\n",
    "checkpoint_dir = '/opt/ml/checkpoints'</p><p>\n",
    "In [ ]:\n",
    "# neuron chache directory\n",
    "cache_dir = \"/opt/ml/checkpoints/neuron_cache\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
